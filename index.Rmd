---
title: "Musicology-Portfolio"
author: "Finn Duijvestijn"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
    theme:
      version: 5

---

```{r}
library(tidyverse)
library(spotifyr)
library(flexdashboard)
library(ggplot2)
library(plotly)
library(compmus)

library(cowplot)
library(patchwork)

library(ggdendro)
library(heatmaply)
library(protoclust)
library(tidymodels)
library(kknn)

Sys.setenv(SPOTIFY_CLIENT_ID="34c9d0d698064bf886a78b343db5445b")
Sys.setenv(SPOTIFY_CLIENT_SECRET="d0e93595f1ee411bb21938c5cb50d247")

access_token <- get_spotify_access_token()

pop <- get_playlist_audio_features("", "2AipD5zTjWWM13wk2hZmej")
phonk <- get_playlist_audio_features("", "37i9dQZF1DWWY64wDtewQt")
rap <- get_playlist_audio_features("", "37i9dQZF1DX76t638V6CA8")

awards <-
  bind_rows(
    pop |> mutate(category = "Pop"),
    phonk |> mutate(category = "Phonk"),
    rap |> mutate(category = "Rap")
    
  )
```




Introduction {.storyboard data-icon="ion-ios-home"}
=========================================

### Exploring the Unique Sounds: A Comparison of Three Distinct Music Genres {data-commentary-width=450}
<font style="font-size: 30px">My Corpus choice</font>

My corpus consists of a collection of songs from various genres including mainly Pop, Rap, and Phonk (which is a sub genre of hip hop and
trap music). These three genres are divided into three playlist for easier comparisons among them. Something interesting to keep in mind: I personally listen to these different genres under various circumstances. For example I tend to listen mostly to Pop when studying, while when working out I like to listen to Rap to give me that extra energy, and finally when gaming I tend to listen mostly to Phonk to focus in a game. Also note that both the Rap and Phonk playlist are from spotify's catalog while the Pop playlist was made by myself. However the Pop playlist is actually more of a mix of popular songs I gathered from over the years. Since most of them are therefore classified as Pop, I will just refer to this playlist as the Pop genre playlist. It will be interesting to see if there are some songs in this Pop playlist which could actually better have been placed in one of the other two playlist according to spotify's API features. So basically this corpus was chosen because I want to explore the differences and similarities among these various genres in terms of
their musical characteristics and find out which songs typically fit into each category. Initially these three main genres within my corpus seem very different, so I thought it would be
interesting to see if there were any specific overlapping musical characteristics which makes them so compelling for my taste in music.

<font style="font-size: 30px">Comparisons</font>

The natural comparison points in my corpus are therefore the different genres: Pop, Rap, and Phonk. I expect to see differences in terms
of rhythm, melody, harmony, and lyrics. For example, I expected Pop music to have a more upbeat rhythm and catchy lyrics compared to Rap
music which might have a heavier beat and very different flowing lyrics. However, I also expected to see some similarities across genres,
for example Rap and Phonk often seem to have a similar type beat and rhythm: heavy and fast paced. It will be interesting to explore the
similarity and differences regarding the instruments and music keys/notes used in typical songs of each genre.

The tracks in my corpus are representative of the groups I want to compare, as I selected them based on their popularity and mainstream
recognition within each genre. However, it is possible that my corpus might not cover all sub-genres within each main genre, and therefore
may not represent the full scope of each genre.

Some examples of typical tracks in my corpus are "Invincible" by Pop Smoke, a popular Rap/Hiphop song, and "Counting Stars" by
OneRepublic, a popular Pop song with its iconic vocal harmonies and theatrical elements. Atypical tracks in my corpus include "Sweater
Weather" by the Neighbourhood, which is a mix between Indie rock and alternative rock, and "break from toronto" by PartyNextDoor, which
could be classified as R&B or Hiphop but also contains a form of rap. These atypical songs may cause there to be some outliers in the data.
These outliers may have to be filtered out first in order to be able to make the most fair comparison for each genre.

In conclusion, this portfolio will focus on exploring and analyzing the musical characteristics of Pop, Rap, and Phonk, to find out what
makes them appealing to listen to under various circumstances.

***

<iframe src="https://open.spotify.com/embed/playlist/2AipD5zTjWWM13wk2hZmej?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe src="https://open.spotify.com/embed/playlist/37i9dQZF1DWWY64wDtewQt?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe src="https://open.spotify.com/embed/playlist/37i9dQZF1DX76t638V6CA8?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


Visual analysis of my corpus {.storyboard data-icon="fa-signal"}
=========================================

### Keys & Genre
```{r}
p <- awards |>
  ggplot(aes(x = key_name, fill = key_name)) +
  geom_bar() +
  facet_wrap(~category) +
  ggtitle("Distribution of keys per genre")

ggplotly(p) %>% layout(height=800)
```


***
**Explanation**

This bar chart gives a good representation of the differences in key usage between the three genres. I was wondering if there would be a unique key pattern for each genre and thus if an extremely different type of key distribution would be visible for each of the three genres. I expected to see a great use of the C keys for pop followed by the G and F keys, since I remembered reading that these typically seem to have the highest usage rates. So it was no surprise that this was actually the case. However I was surprised to see how balanced the key distribution of the Pop genre was, since the other genres seem to have a lot more imbalances. Furthermore I expected the Rap genre to have the highest usage of the E key, followed by the A and G keys. However to my surprise it was actually the C# key, and the other keys did not even come close to this usage rate. Since Phonk has roots of Trap and Hip-hop I expected to see a lot of use of the F or G keys. For the F key this was absolutely false, however for the G key it was relatively true since there was a lot of usage of this key. Although it was definitely not the most used key which actually was the C#. 

So something interesting to look at is the most used key per genre, for example both Phonk and Rap make use of the C# key the most. I did expect similar key usage in Phonk and Rap since the style of beat seems similar. However it seems that in Rap songs, this key is used almost twice as much as any other key on average, while in Phonk is seems to be relatively balanced with the other keys. It is also interesting to see that Phonk does not have a single instance of the D# key. Also we can clearly see that out of the three genres, Pop seems to have the most balanced distribution of key usage since it uses all keys and does not really have one major outstanding key.


### Histogram of Tempi

```{r}
# Change histogram plot fill colors by groups, Use semi-transparent fill
p <- awards |>
  ggplot(aes(x = tempo, fill = category)) +
  geom_histogram(alpha = 0.8) +
  scale_fill_brewer(palette="PuRd") +
  ggtitle("Distribution of tempi for: Phonk vs Pop vs Rap")

ggplotly(p) %>% layout(height=800)
```


***
**Explanation**

I thought it would be interesting to have a look at the different tempi for each of the three genres. I expected to see a lot of similarities between them, since most songs I like and listen to typically  seem to have a tempo within a certain range. I do not like extremely fast paced songs because it is hard to track the actual musical characteristics or sing/dance along. However I also don’t like extremely slow music since I tend to get bored easily from song which are classified as slow-songs.

Now for the actual differences in the tempi of the genres, interestingly even though both Phonk and Pop seem to have a preferred tempo between 120 and 130 beats per minute (BPM), the plot shows that Rap likes to have a little faster tempo on average: around 140 BPM. However the plot also does show a slight peak at 140 BPM for both Phonk and Pop. Since the rap playlist includes the least amount of songs out of the three genres, it seems that overall most songs I like are still around 120-140 BPM. 
Some other interesting thing to see is, that aside from the 140 BPM range , all three genres also have a small peak at around 95 BPM, which is a lot slower than the usual tempo of these genres. This could mean two things, the first one being that is does in fact not matter whether the tempo of a song is extraordinary for a specific genre. The song could still thrive in a genre regardless of the tempo if the song is still composed very well. The second cause could be that these peaks are caused by sub-genres within each of the main three genres, which could then mean that there are some overlapping sub-genres within the three main genres. It would be interesting to look deeper into the sub-genres within each main genre and see if there is some overlapping there which may have caused this kind of behavior.


### Energy & Valence
```{r}
p <- awards |>                    # Start with awards.
  ggplot(                     # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = loudness,
      colour = energy
    )
  ) +
  geom_point() +              # Scatter plot.
  geom_rug(linewidth = 0.2) + # Add 'fringes' to show data distribution.

facet_wrap(~ category) +    # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_size_continuous(      # Fine-tune the sizes of each point.
    trans = "exp",            # Use an exp transformation to emphasise loud.
    guide = "none"            # Remove the legend for size.
  ) +
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Valence",
    y = "Energy",
  ) + ggtitle("Relationship between loudness, energy and valence in Phonk, Rap & Pop")

ggplotly(p) %>% layout(height=800)
```


***
**Explanation**

For this plot I decided to compare the valence and energy of The three genres: Pop, Rap and Phonk. Valence measures the positivity of a song's musical content, while energy measures the intensity and activity of a song. These are two important factors that contribute to a song's overall mood and emotional impact. I wanted to see if there was any correlation between the valence and the energy of songs. I thought it would also be interesting to see if the songs of these different genres would differ massively in overall mood and emotional impact. 

The distribution interestingly immediately shows that like for the distribution of keys, Pop songs seem to be the most balanced and equally distributed. While Phonk songs all seem to lie towards the top of the energy scale and Rap songs seem to lie more around the middle of the energy scale. I expected the Pop genre to have the highest average valence out of the three genres, since these songs are often linked to positive vibes. And Phonk to have the highest energy values on average, since this genre is associated with fast, loud and noisy music (as can be seen in the plot where bigger points indicate louder songs). Therefore it was kind of surprising to see that pop had actually a really balanced valence distribution. What surprised me even more was to see that this was the case for all three genres, they were all pretty balanced towards the valence scale. Although it was no surprise that Phonk had a relatively high energy on average, I was surprised to find out that the Phonk genre actually also scored relatively high on average for valence too, since I assumed this fast and loud type of music would give off an angry vibe. I also expected to see Rap quite high on the energy scale since there are a lot of fast paced heavy beat rap songs out there, but this was not exactly the case. At least by comparison to the other two genres Rap really seemed to have a slightly lower energy score on average. However Rap songs do seem to have a way higher correlation between energy and valence, since this is the only graph where really a clear linear trend can be seen between these two measurements. This shows that there could be a correlation between the valence and energy of songs, however it is very dependent on the genre it seems.


### Dancebility

```{r}
p <- awards |>
  ggplot(aes(x = category, y = danceability, fill = category)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_brewer(palette="PuRd") +
  ggtitle("Dancebility: Phonk vs Pop vs Rap")

ggplotly(p) %>% layout(height=800)
```


***
**Explanation**

This boxplot shows that interestingly enough, rap seems to have the highest danceability value on average according to Spotify’s measure. Danceability is a measure of how suitable a track is for dancing, based on factors such as rhythm stability, beat strength, and overall tempo.

I would personally have guessed that Pop songs would have the highest danceability by a mile, since Pop music is often associated with upbeat, danceable tracks that are popular in clubs and on dancefloors. So seeing that Pop only had a median score of 0.69 for danceability, while Phonk had a median of 0.70 and Rap even a median as high as 0.79, was really surprising to me. I expected that Rap and Phonk music would have prioritized other musical characteristics over danceability, such as lyricism, groove, or atmospheric qualities. I also thought danceability could be influenced by energy scores, which is why I expected Phonk to do quite well. However, as can be seen in the graph, Rap wins the danceability battle by quite a big margin. I think it is really interesting to see Rap score so high, especially considering that rap had really low energy levels in comparison to the other two.



### Tempo & Genre

```{r}

fr <- pop |> slice(1:50) |>
  add_audio_analysis()

qu <- phonk |> slice(1:50) |>
  add_audio_analysis()

qu2 <- rap |> slice(1:50) |>
  add_audio_analysis()

comp <-
  fr |>
  mutate(category = "Pop") |>
  bind_rows(qu |> mutate(category = "Phonk"), qu2 |> mutate(category = "Rap"))
comp <- comp %>% rename(track = track.name)

Tempo <- comp |>
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections)

TempoAnimated <- Tempo %>%
  plot_ly(x = ~tempo, y = ~tempo_section_sd, color = ~tempo, size = ~loudness 
    ,hoverinfo = "text", text = paste("Track: ", Tempo$track, "<br>",
                          "Genre: ", Tempo$category, "<br>",
    #                       "Release date: ", Tempo$track.album.release_date, "<br>",
                          "Tempo: ", Tempo$tempo, "<br>",
                          "Loudness: ", Tempo$loudness, "<br>")
                          # "Time signature: ", Tempo$time_signature)
  ) %>%
  add_markers(frame = ~category) %>%
  layout(title = 'Tempo for Each Genre', xaxis = list(title = "Mean Tempo (bpm)"), yaxis = list(range = c(0,10), title = 'SD Tempo')) %>%
  animation_opts(frame = 2000, 
                 transition = 100,
                 easing = "linear") %>%
  animation_slider(currentvalue = list(prefix = NULL,
                                       font = list(color = "a9dea9", size = 20)))

ggplotly(TempoAnimated) %>% layout(height=800)
```


***
**Explanation** 

Here you can see the mean tempo, the deviation of each song from its mean tempo, the loudness in the size of the datapoint and the bpm also in the color of the point for extra clarity. The slider can be moved to display all the elements of the selected genre, or the play button can be pressed to display each genre after each other like a slideshow.

This graph is useful to see why the histogram of tempi was distributed the way it was, as well as identifying all outliers which may have caused deviations from the standard tempo range for a genre. It is also intriguing to examine songs that exhibit variations in tempo within the same piece and determine the number of standard deviations by which this deviation differs from the mean tempo.

It is interesting to see that Phonk can almost be divided into three tempo ranges: around 100 BPM, around 120 BPM and around 140 BPM. This really does explain the histogram of tempi well for this genre. With the song [help urself](https://open.spotify.com/track/1lethytswFEKkvfNIjdCC1?si=8c70d4afbb7b4766) being the outlier for slowest average tempo of around 82 BMP and the song [METAMORPHOSIS](https://open.spotify.com/track/6MlIIJwO4FxnOlrpOrS4hU?si=30b8eb9c69c34512) being the outlier for fastest average tempo of around 175 BMP. 

It is interesting to see that Pop seems to have the most stable bpm range: more than 50% lies within the 120-140 bpm range and spreads out evenly on both sides. It also really only has one major outlier, the song [Fireflies](https://open.spotify.com/track/3DamFFqW32WihKkTVlwTYQ?si=8728d99d82f24c86) has an extremely fast tempo of around 180 BPM.

Something fascinating is that all songs of Pop and Phonk do not really deviate a lot from the main tempo within the piece, while there are some extreme deviations within rap songs. One example of this is the song [family ties]( https://open.spotify.com/track/3QFInJAm9eyaho5vBzxInN?si=84e4f553400e459c), which has a tempo STD of around 9.5. It also seems that Rap songs are all over the place when it comes down to mean tempo, so there are not really any major outliers here it seems.



### Timbre & Genre
```{r}
viopl <- comp |>
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) |>
  select(category, timbre) |>
  compmus_gather_timbre() |>
  ggplot(aes(x = basis, y = value, fill = category)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Genre")

ggplotly(viopl) %>% layout(height=800)
```


***
**Explanation**

Timbre is a term used to describe the unique tonal quality or "color" of a sound, and timbral analysis can provide insights into the spectral and temporal characteristics of different genres of music. The 12 timbre coefficients measured by the Spotify API are based on Mel-frequency cepstral coefficients (MFCCs), which are commonly used in speech and music recognition to capture the spectral features of audio signals. These coefficients can help distinguish between different types of sounds based on their harmonic content, brightness, and other factors.

Here is the explanation of each of the 12 coefficients:
"c01": This coefficient represents the overall brightness or darkness of the audio segment, based on the spectral energy distribution.
"c02": This coefficient represents the spectral centroid, which is a measure of the center of gravity of the spectral energy distribution. It is related to the perceived "brightness" or "darkness" of the sound.
" c03": This coefficient represents the spectral spread, which is a measure of the width of the spectral energy distribution. It is related to the perceived "sharpness" or "dullness" of the sound.
" c04": This coefficient represents the spectral skewness, which is a measure of the asymmetry of the spectral energy distribution. It is related to the perceived "smoothness" or "roughness" of the sound.
" c05": This coefficient represents the spectral kurtosis, which is a measure of the "peakedness" of the spectral energy distribution. It is related to the perceived "clarity" or "muddiness" of the sound.
" c06": This coefficient represents the spectral flatness, which is a measure of the relative balance of energy across the frequency spectrum. It is related to the perceived "tonality" of the sound.
" c07",  " c08", " c09", " c10", " c11" and " c12": all these coefficients capture the overall spectral shape of the sound within each frequency range.

Something interesting to see, is that the first coefficient is not very diverse since it does not have a lot of deviation from its mean value. This could be because the first coefficient, "c01," which represents the overall brightness or darkness of the audio segment, captures a more general aspect of the audio signal. It represents the average energy distribution across the entire frequency spectrum, rather than focusing on specific frequency ranges or spectral characteristics. However the opposite is true for the rest of the first half of the coefficients, they have a lot of deviation, while the second halve tends to get more and more dense and deviate less.	



Chroma features {.storyboard data-icon="ion-music-note"}
=========================================

### Chromatograms

<!-- <font style="font-size: 30px">1. Counting Stars -- 2. Close Eyes -- 3. Invincible</font> -->

```{r pop-chroma}
ewf <-
  get_tidy_audio_analysis("2tpWsVSb9UEmDRxAl1zhX1") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
loveontop <-
  get_tidy_audio_analysis("3CLSHJv5aUROAN2vfOyCOh") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
allineed <-
  get_tidy_audio_analysis("792HwhrdO3ErRKL5yRe4Ge") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
```

```{r pop-chroma-plots}
ewf_plot <- ewf |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
    geom_vline(xintercept = 21.5, color = "red") +
    geom_vline(xintercept = 77.5, color = "red") +
    geom_vline(xintercept = 206.5, color = "red") +
  # labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Pop: 'Counting Stars' by 'OneRepublic'") +
  # labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  # theme_minimal() +
  scale_fill_viridis_c(option = "mako") 

loveontop_plot <- loveontop |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 70, color = "red") +
  # labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Phonk: 'Close Eyes' by 'DRVST'") +
  # labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme(legend.position="none") +
  scale_fill_viridis_c(option = "mako") 

allineed_plot <- allineed |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 23.5, color = "red") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "1. Pop: Counting Stars - OneRepublic, 2. Phonk: Close Eyes - DRVST, 3. Rap: Invincible - Pop Smoke") +
  theme(legend.position="none") +
  scale_fill_viridis_c(option = "mako") 

subplot(ewf_plot, loveontop_plot, allineed_plot, nrows = 3) %>% layout(height=900)
# plot_grid(ewf_plot, loveontop_plot, allineed_plot, ncol = 1)
# ewf_plot / loveontop_plot / allineed_plot
```


***
**Explanation**

A chromagram is a visual representation of the distribution of pitches in a musical recording. The x-axis represents the time in seconds and the y-axis represents the pitch.

The first chromatogram shows that for the Pop song: [Counting Stars]( https://open.spotify.com/track/2tpWsVSb9UEmDRxAl1zhX1?si=cb3fd758427648ee) by *OneRepublic*, the C# pitch has the longest time use by far and it seems that C comes second. The rest of the pitches seem to get less playtime in comparison. We can clearly see that after around 21 seconds (indicated by the first red line), there are some new pitches introduced. This is correct since this is the end of the intro of the song and all the instruments slowly start playing one after another. At around 78 seconds into the song (indicated by the second red line), the same thing happens: after this ending a section again all instruments start playing again. The last time this happens is at around 206 seconds, and you can really clearly hear the point where the music introduces all the instruments again.

The second chromatogram shows that for the Phonk song: [Close Eyes]( https://open.spotify.com/track/3CLSHJv5aUROAN2vfOyCOh?si=bb8f1799d5e3433d) by *DRVST*, the C pitch has the longest time use by far and it seems that the C#, F and E pitches follow regarding playtime. The rest of the keys do not seem to get much playtime in comparison. At around the halfway point in the song, we can hear a short pause from almost all heavy instruments before continuing the song. This is indicated with the red line at a time of around 70 seconds.

The third chromatogram shows that for the Rap song: [Invincible]( https://open.spotify.com/track/792HwhrdO3ErRKL5yRe4Ge?si=0609de95cdf94123) by *Pop Smoke*, the F# pitch has the longest time use. We can exactly tell where the beat drops and the bass is introduced, which is at around 23 second indicated by the red line again. We can clearly see the introduction of the high intensity F# pitch at that point.

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2tpWsVSb9UEmDRxAl1zhX1?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3CLSHJv5aUROAN2vfOyCOh?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/792HwhrdO3ErRKL5yRe4Ge?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>



### Cepstrograms


```{r timbre_pop}
# Cepstrograms
bzt <-
  get_tidy_audio_analysis("1XGmzt0PVuFgQYYnV2It7A") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

cep_pop <- bzt |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +
  theme_classic()+
  ggtitle("1. Pop: Payphone - Maroon 5, 2. Phonk: Sahara - Hensonn, 3. Rap: Yonkers - Tyler, The Creator")
```


```{r timbre_phonk}
# Cepstrograms
bzt <-
  get_tidy_audio_analysis("6nqdgUTiWt4JbABDurkxMI") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

cep_phonk <- bzt |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +
  theme_classic()
```


```{r timbre_rap}
# Cepstrograms
bzt <-
  get_tidy_audio_analysis("3WQlJpaUUbGtUqAskvGA7c") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

cep_rap <- bzt |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))

subplot(cep_pop, cep_phonk, cep_rap, nrows = 3) %>% layout(height=900)
```


***
**Explanation** 

Cepstrograms, also known as cepstral spectrograms, are a type of spectrogram where the x-axis represents the time in seconds and the y-axis represents the cepstral coefficients instead of the frequency. The ceptrograms show the changes regarding timbre of typical songs from each genre: Pop, Phonk and Rap.

Comparing the ceptrograms of three songs, [Payphone]( https://open.spotify.com/track/1XGmzt0PVuFgQYYnV2It7A?si=056f611363034241) from the Pop genre, [Sahara]( https://open.spotify.com/track/6nqdgUTiWt4JbABDurkxMI?si=b41f0709602344bc) from the Phonk genre and [Yonkers]( https://open.spotify.com/track/3WQlJpaUUbGtUqAskvGA7c?si=095624db638d4349) from the rap genre, reveals interesting differences in their timbral distribution. We can immediately tell that all three songs have a clearer timbral structure towards the lower end of the coefficients, however there are some small differences between the songs. 

In “Payphone” for example there is a relatively strong concentration of timbral characteristics around the second coefficient after the like the first 50 seconds, indicating a strong "brightness" or "darkness" of the sound in the areas where bright yellow accents can be seen. 

The opposite is true for the coefficients in "Sahara",  there seems to be a strong concentration of timbral characteristics around the second coefficient only for like the first 25 seconds, indicating a strong "brightness" or "darkness" of the sound in the areas where bright yellow accents can be seen.

On the other hand, both “Yonkers” shows more variation in timbral distribution, with a wider range of timbral characteristics. It is only in the last 50 seconds that we really see the most concentration of the second timbral coefficient. This could suggest that this song has a more complex and varied sound texture. These differences in timbral distribution could contribute to the overall appeal of the songs and could be further explored in future analyses.

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/1XGmzt0PVuFgQYYnV2It7A?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/6nqdgUTiWt4JbABDurkxMI?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3WQlJpaUUbGtUqAskvGA7c?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>


### Self-similarity matrices

```{r self sim, echo=FALSE}
norms_chroma <- list("euclidean", "manhattan", "chebyshev")
dists_chroma <- list("manhattan", "aitchison", "cosine", "angular")
summary_chroma <- list("mean", "aitchison_centre", "root_mean_square", "max", "acentre")

norms_timbre <- list("euclidean", "manhattan", "chebyshev")
dists_timbre <- list("euclidean", "cosine", "angular")
summary_timbre <- list("mean", "root_mean_square")

trumpets <-
  get_tidy_audio_analysis("6jizk5lOUnfpaZXYMdfeC6") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = summary_chroma[5], norm = norms_chroma[3]
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = summary_timbre[1]
      )
  )

metamorphosis <-
  get_tidy_audio_analysis("2ksyzVfU0WJoBpu8otr4pz") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = summary_chroma[5], norm = norms_chroma[3]
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = summary_timbre[1]
      )
  )

still <-
  get_tidy_audio_analysis("503OTo2dSqe7qk76rgsbep") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = summary_chroma[5], norm = norms_chroma[3]
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = summary_timbre[1]
      )
  )
```

```{r self sim matrices, echo=FALSE}
self_sim1 <- bind_rows(
  trumpets |> 
    compmus_self_similarity(pitches, dists_chroma[1]) |> 
    mutate(d = d / max(d), type = "Chroma"),
  trumpets |> 
    compmus_self_similarity(timbre, dists_timbre[2]) |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "mako", guide = "none") +
  theme_classic()

self_sim2 <- bind_rows(
  metamorphosis |> 
    compmus_self_similarity(pitches, dists_chroma[1]) |> 
    mutate(d = d / max(d), type = "Chroma"),
  metamorphosis |> 
    compmus_self_similarity(timbre, dists_timbre[2]) |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "mako", guide = "none") +
  theme_classic()

self_sim3 <- bind_rows(
  still |> 
    compmus_self_similarity(pitches, dists_chroma[1]) |> 
    mutate(d = d / max(d), type = "Chroma"),
  still |> 
    compmus_self_similarity(timbre, dists_timbre[2]) |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "mako", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "", title = "1. Pop: Trumpets - Jason Derulo, 2. Phonk: METAMORPHOSIS - INTERWORLD, 3. Rap: Still D.R.E. - Dr. Dre, Snoop Dogg")

subplot(self_sim1, self_sim2, self_sim3, nrows=3) %>% layout(height=900)
```


***

**Explanation**

A self-similarity matrix is a visual representation of the similarity between different sections of a musical recording. The two self-similarity matrices, each summarized at the bar level but with axes in seconds, illustrate pitch- and timbre-based self-similarity within the songs: [Trumpets](https://open.spotify.com/track/6jizk5lOUnfpaZXYMdfeC6?si=9f8d4be6f1074bc2) from *Jason Derulo*,  
[METAMORPHOSIS]( https://open.spotify.com/track/2ksyzVfU0WJoBpu8otr4pz?si=9bacd80184f94342) from *INTERWORLD* and 
[Still D.R.E.]( https://open.spotify.com/track/503OTo2dSqe7qk76rgsbep?si=730c0c0a1c6f4743) from *Dr. Dre, Snoop Dogg*,  These songs were chosen because I think it really shows the timbre and pitch based structure of each genre well.

First of all we can see that each song is of different length, since these matrices are not the same size. We can identify repeated or similar musical motifs by looking for diagonal patterns in the SSM. The degree of similarity between the motifs can be estimated from the height and width of the diagonal patterns, as well as their intensity or color. Both "Trumpets" and "METAMORPHOSIS" seem to have a checkerboard pattern, which typically indicates a lack of similarity or structure in the musical piece. This pattern can occur when the musical piece has no consistent motif or structure, or when it is composed of different sections with little or no relation to each other. However it can also just mean that its features are constant over a duration of time. It is also interesting to see that that while their chroma similarity matrices look alike, their timbre similarity matrices look relatively different.

"Sill D.R.E" seems to only have repeating horizontal and vertical stripes without a clear pattern. This happens when the musical piece has a repetitive structure, with repeated motifs or sections that are similar to each other.

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/6jizk5lOUnfpaZXYMdfeC6?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2ksyzVfU0WJoBpu8otr4pz?utm_source=generator&theme=0" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/503OTo2dSqe7qk76rgsbep?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>





### Keygram of *XXXTENTACION : SAD!*

```{r keygram setup, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}
#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)
major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r keychram corpus setup, echo=FALSE}
sad <-
  get_tidy_audio_analysis("3ee8Jmje8o58CHK66QrVC2") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r keygram, echo=FALSE}
sad1 <- sad |>
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  ggtitle("SAD! - XXXTENTACION") +
  labs(x = "Time (s)", y = "")
ggplotly(sad1) %>% layout(height=800)
```


***
**Explanation**

This is a keygram from one of the last songs from *XXXTENTACION*: [SAD!](https://open.spotify.com/track/3ee8Jmje8o58CHK66QrVC2?si=450dd927a69547c9) brought out before he died the 18th of june 2018. It is interesting because this song is from my Pop playlist while it is actually classified as more of a Hiphop/trap/R&B song. I was interested to see if the keygram shows abnormalities compared to a normal Pop song.

However as can be seen the key estimates are really blurry, this means that the algorithm has a hard time finding a stable estimation of what the actual key usage is. The sparser texture of the song throws off the key-finding algorithm seriously. I am wondering what makes this song especially hard for the algorithm to structure, since in comparison to other songs I found that this song had one of the least clear key estimates. I think this does make the musical characteristics of the song more interesting to analyze further in the future. So in conclusion the difference between this song and songs actually classified as Pop songs, is that Pop songs tend to have a little clearer structure while this song :*SAD!* is especially unclear in its keygram.

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3ee8Jmje8o58CHK66QrVC2?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>


### Two **Mozart** chordograms: *Piano Sonata No. 11* and *MOZART PHONK*

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}
#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)
major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
major_key <-
  c(5.0, 2.0, 3.5, 2.0, 4.5, 4.0, 2.0, 4.5, 2.0, 3.5, 1.5, 4.0)
minor_key <-
  c(5.0, 2.0, 3.5, 4.5, 2.0, 4.0, 2.0, 4.5, 3.5, 2.0, 1.5, 4.0)
chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

mozart_phonk <-
  get_tidy_audio_analysis("4uA4FfwW15P8PguZ1uOlpl") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
actual_mozart <-
  get_tidy_audio_analysis("3NZ8O0jXxviUvqcqnQ3FZV") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

ph_moz_chplt <- mozart_phonk |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal()
  # labs(title = "MOZART PHONK", x = "", y = "") +
  # theme(axis.text.y = element_text(size = 7), plot.margin=unit(c(5.5, 5.5, 0, 5.5), "pt"))

ac_moz_chplt <- actual_mozart |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "MOZART PHONK vs Piano Sonata No. 11", x = "Time (s)", y = "") 
  # theme(axis.text.y = element_text(size = 7), plot.margin=unit(c(5.5, 5.5, 0, 5.5), "pt"))

subplot(ph_moz_chplt, ac_moz_chplt, nrows = 2) %>% layout(height=900)
```


***
**Explanation**

In the Phonk playlist there is the song: [MOZART PHONK](https://open.spotify.com/track/4uA4FfwW15P8PguZ1uOlpl?si=a6f5ace5b3984bf8), from which the chordogram can be seen in the top plot. I thought It would be fun to compare the chordogram of that song with the actual song from Mozart: [Piano Sonata No. 11](https://open.spotify.com/track/3NZ8O0jXxviUvqcqnQ3FZV?si=7cbecb9ff2fe42d6), which can be seen in the plot below. The x-axis represents the time in secends whereas the y-axis represents the chords played at those times.

We can see that some chord structure of the original song is kept, even though a couple of lot chords might have been altered or completely changed for extra energetic effect. It seems that the MOZART PHONK has a higher coverage of bright yellow squares, indicating that there are a lot of chords with extremely high energy. While the original song seems to have a little more balance between high energy chords and low energy chords. However when zooming in on the original song, so that it's domain is ranging from 0-70 seconds, we can see that the two songs are really quite similar in regard to their chord profile. So although I believe it is slightly difficult to compare the two songs since the duration of the original song is a lot longer, we can still clearly see a lot of resemblance of the original song in the Phonk version of the song.


<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/4uA4FfwW15P8PguZ1uOlpl?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/3NZ8O0jXxviUvqcqnQ3FZV?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>



### temporal features

```{r}
tempf <- get_tidy_audio_analysis('5fEB6ZmVkg63GZg9qO86jh') |> 
    tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |> 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
    ggtitle("Break from Toronto - PARTYNEXTDOOR") +
    labs(x = 'Time (s)', y = 'Tempo (BPM)') +
    theme_minimal()
ggplotly(tempf)
```


***
**Explanation**

The Fourier-based tempogram at the left is an attempt to use Spotify's API to analyse the tempo of _PARTYNEXTDOOR_: [Break from Toronto](https://open.spotify.com/track/5fEB6ZmVkg63GZg9qO86jh?si=83b0b977075048e2) Overall, Spotify estimates that the tempo of this track is 117 BPM, looking in more detail at the tempogram, we can see that this makes sense.The bright yellow line does indeed lie around 117 BPM, however we can also see that there are some indicators that higher and lower tempos are also included in the beginning. These different sections of the piece lead to different strengths of so-called *tempo octaves*. This freer-form section of the piece can make it so tempo estimation is a little harder.

This song could be classified as one of the outliers in my Pop playlist, since the genre is actually more R&B/Soul, however the song still only falls slightly below the average tempo for a Pop song namely: 120-130 BPM. This can be seen in the histogram of tempi on the visual analysis page.

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/79MSEdtXuudhGhC5AtG07g?utm_source=generator" width="100%" height="100" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>



Classification {.storyboard data-icon="fa-pie-chart"}
=========================================

### Dendogram & heatmap

```{r startup}
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  
get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
} 


indie <-
  bind_rows(
    pop |> mutate(playlist = "Pop") |> slice_head(n = 20),
    phonk |> mutate(playlist = "Phonk") |> slice_head(n = 20),
    rap |> mutate(playlist = "Rap") |> slice_head(n = 20),
  ) |> 
  add_audio_analysis()

indie_features <-
  indie |>
  mutate(playlist = factor(playlist)) |>
  mutate(segments = map2(segments, key, compmus_c_transpose)) |>
  mutate(
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

indie_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = indie_features
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors())

indie_cv <- indie_features |> vfold_cv(5)

knn_model <-
  nearest_neighbor(neighbors = 1) |>
  set_mode("classification") |> 
  set_engine("kknn")
```


```{r halloween}
halloween <-
  get_playlist_audio_features("", "2AipD5zTjWWM13wk2hZmej") |>
  slice(1:20) |>
  add_audio_analysis() |>
  mutate(segments = map2(segments, key, compmus_c_transpose)) |>
  mutate(
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))
halloween_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = halloween
  ) |>
  step_range(all_predictors()) |>
  prep(halloween |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")
ggheatmap(
  halloween_juice,
  hclustfun = protoclust,
  dist_method = "manhattan")
```

***

**Explanation** 

The dendrogram and heatmap breaks the Pop playlist into small clusters and shows us which musical features are most intense for each song. These musical characteristics/features are on the x-axis and the songs themselves can be seen on the y-axis. It consist of the first 20 songs of the Pop playlist since it would not be readable to apply a dendrogram for the entire playlist. Since these songs all belong to the same genre/playlist I wanted to see if the algorithm could subdivide them even more. 

Something I found interesting, is that it doesn't seem that the algorithm takes into account which artist is on the song. This is because it can be seen that Post Malone has namely two songs within the dendrogram which are not on the same subcluster: "Wow" and "Goodbyes". Although both "Wow" and "Goodbyes" were created by Post Malone and belong to different subclusters within the same main cluster, their shared membership in the main cluster still suggests a degree of relationship between them. 


### Ranking important features

```{r features}

forest_model <-
  rand_forest() |>
  set_mode("classification") |> 
  set_engine("ranger", importance = "impurity")
indie_forest <- 
  workflow() |> 
  add_recipe(indie_recipe) |> 
  add_model(forest_model) |> 
  fit_resamples(
    indie_cv, 
    control = control_resamples(save_pred = TRUE)
  )

# indie_forest |> get_pr()

workflow() |> 
  add_recipe(indie_recipe) |> 
  add_model(forest_model) |> 
  fit(indie_features) |> 
  pluck("fit", "fit", "fit") |>
  ranger::importance() |> 
  enframe() |> 
  mutate(name = fct_reorder(name, value)) |> 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")
```


***

**Explanation**

Random forests are a powerful variant of the decision-tree. Although no single classifier works best for all problems, in practice, random forests are among the best-performing off-the-shelf algorithms for many real-world use cases. Random forests also give us a ranking of feature importance, which is a measure of how useful each feature in the recipe was for distinguishing the ground-truth classes.

Since random forests also give us a ranking of feature importance, I decided to use this to find out which features ranked highest for distinguishing whether a song was part of the Pop, Phonk or Rap genre. As can be seen there are two features which seem to have a major impact on deciding to which genre a song belongs: Instrumentalness and duration. This makes sense since almost all Phonk songs for example have a relatively short duration of the song, but are very instrumental. Interestingly the rest of the rest of the features seem to not play as big of a role in this decision.



### Confidence Matrix

```{r workouts}
indie_knn <- 
  workflow() |> 
  add_recipe(indie_recipe) |> 
  add_model(knn_model) |> 
  fit_resamples(indie_cv, control = control_resamples(save_pred = TRUE))

ConfMatAll <- indie_knn |> get_conf_mat()
AllPrecision <- indie_knn |> get_pr()

ConfMatAll |> autoplot(type = "heatmap")
```


***
**Explanation**

Based on the features which can be seen on the previous page, I made a model to predict whether a song belongs in the Pop genre, the Phonk genre or the Rap genre. Again the features that were the most important were: Instrumentalness and duration. 

Here you can see how well the model did; the number in the cells show for every combination (the three true categories and how the three categories were predicted) how often this was the case. For example, the first value (upper left corner) shows the number of times the model predicted correctly that a song was from the Phonk genre, the value immediately below shows for every Phonk song the number of times it predicted it was a Pop song and underneath that is the times that it predicted it as a Rap song. Surprisingly classifying Pop songs seems to be the easiest for the algorithm. Songs from both the Phonk and Rap genre seem to be harder to classify for the algorithm since these have a lot less correct predictions.

Below is a table of the actual evaluation metrics showing the performance of the classifier.

**Evaluation model using all features**  

```{r}
AllPrecision %>% knitr::kable()
```

Precision is the proportion of accurately predicted songs from all songs the model predicted to be from that genre. Recall shows from all songs that are actually from that genre the proportion that got predicted accurately. 



### Two most important features & Genre

```{r}
indie_features |>
  ggplot(aes(x = duration, y = instrumentalness, colour = playlist, size = energy)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Duration",
    y = "Instrumentalness",
    size = "Energy",
    colour = "Playlist"
  )
```

***

**Explanation**

Since the feature ranking showed us that Instrumentalness and Duration were the two most important features for classifying a song to be within one of the three genres: Pop, Phonk and Rap, I decided to make a more interpretable plot to be able to see the actual differences between the genres for those exact features.

Interestingly Rap songs seem to have almost no instrumentalness on average. What surprised me even more is that besides only three Pop songs, it also seems that Pop songs have very low instrumentalness on average. There does not seem to be a lot of distinction between the duration of these two genres as well. So basically both Rap and Pop songs have low instrumentalness while also having a evenly distributed song duration. This is probably why the classifier finds it hard to predict and classify between these genres.

However to see that Phonk had a high instrumentalness and short duration on average was not surprising to me, like already mentioned in on the previous pages.




What did I find {.storyboard data-icon="fa-spotify"}
=========================================


### Conclusion {data-commentary-width=450}

<font style="font-size: 30px">Wrap up of my corpus analysis</font> 

To conclude the analysis of my corpus, I would like to highlight a few key takeaways that I gained from creating this portfolio. I expected a lot of differences between all of the musical characteristic from the three genres: Pop, Phonk and Rap. As I mentioned in the introduction of my portfolio, I listen to the songs from these various genres under different circumstances. Therefore I would’ve guessed that there would be a lot of different features of these songs, which would make them appealing to listen to during different activities. However to my surprise I found out that the these differences were mostly only slight changes in either tempo, timbre or pitch. It was really interesting to find out that all these little changes in songs added up to make a song fit into a certain genre instead of all in the same one.

First of all seeing the histogram of the different keys used for each genre and seeing that there were big differences, I expected all other measurements to have the same outcome. This expectation turned out to be not really accurate for all musical characteristic comparisons.

One example of these small differences was when analyzing the different tempi of each genre, they all seemed to fall almost perfectly into the same range with an average of 120-140 BPM. Where the difference as little as 5 BPM on average was the difference between complete different genres. But when looking deeper into it, I found that the most noticeable thing wasn’t the difference of the average tempo between genres. It was the distribution of songs for each genre and the deviation of tempo within the songs itself. For example we saw that the Rap genre had some major outliers which were deviating from the mean tempo of the song. While the Pop songs where distributed with almost like a normal distribution type shape. On the other hand, Phonk songs could almost be dived into one of three tempos.

Furthermore I expected to see a lot more similarities between Rap and Phonk, since both of these tend to have a similar type beat and rhythm: heavy and fast paced. While this was sometimes the case, it definitely was not always true. For example, while the energy of Phonk was really high on average, the energy of Rap was way lower than I expected it to be especially in comparison to Pop for example. It was still not low overall but compared to the genres I was analyzing it just seemed lower than expected. 

Something else noticeable was seeing that according to the spotify API, the Rap genre includes the most danceable songs on average. This was unexpected too,  since Pop is know for it’s dancing songs. Not only was Rap the most danceable genre, Pop was actually the least danceable. 

Interestingly I did find that for most comparisons Pop was overall the most balanced and evenly distributed with its features. The spread of datapoints was usually the highest which would lead to a more balanced representation of songs. I also felt like the Phonk genre was usually the least balanced with the least amount of spread in it’s datapoints. Rap seemed to fall somewhere in the middle of the two Genres generally speaking.

Moreover I found it interesting to see how all of the chroma features had different impacts on the structure of songs and how it could effect the way a music piece was perceived. While not all songs had a truly clear structural representation for each chroma representation, like for the keygram, it was still interesting to see the differences between songs from each genre.

All in all  have learnt about my taste in music and the different type of songs I listen to. I have gained a lot of insight into the different musical characteristics which play major roles into shaping music pieces into the way they are and now also now which ones are important in the genres I listen to.

***

```{r picture, out.width = '100%', out.height = '100%'}
knitr::include_graphics("pics/pop.jpg")
knitr::include_graphics("pics/phonk.jpg")
knitr::include_graphics("pics/rap.jpg")
```


### Discussion {data-commentary-width=450}

<font style="font-size: 30px">What's next?</font>

While we have analyzed the playlist data and have found that there are several overlapping features and differences between the different genres, it is important to note that these playlists are constantly updating, so they may not be entirely accurate in the future.

One interesting aspect to consider when analyzing the overlapping songs between genres is the subgenres within each main genre. For example, within the broad category of "Rap" there are many subgenres such as Gangsta rap, Alternative hip hop, and even Trap, which coincidentally also has a lot overlapping features with Phonk.

Analyzing the subgenres within each main genre could shed light on some of the overlapping songs and outliers in the data. It is possible that some of the overlapping songs belong to subgenres that are closely related, while others may be outliers that do not fit neatly into any one category. By analyzing the subgenres, we may be able to gain a more nuanced understanding of the relationship between different genres and their respective fan bases.

Furthermore, analyzing subgenres could also help to identify emerging trends and new subgenres that are gaining popularity. For example, the rise of subgenres such as "indie pop" and "trap music" in recent years has reshaped the landscape of popular music and could be reflected in the Pop playlist data.

However, it is important to note that analyzing subgenres would require a more granular approach to data collection and analysis, which may not be feasible or practical for all research projects. Nonetheless, it is an interesting avenue for future research and could lead to a deeper understanding of the complex relationships between different genres and their respective subcultures.

***

```{r picture2, out.width = '100%', out.height = '100%'}
knitr::include_graphics("pics/pop.jpg")
knitr::include_graphics("pics/phonk.jpg")
knitr::include_graphics("pics/rap.jpg")
```


