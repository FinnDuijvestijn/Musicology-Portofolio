---
title: "Musicology-Portfolio"
author: "Finn Duijvestijn"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
    theme:
      version: 5

---

```{r}
library(tidyverse)
library(spotifyr)
library(flexdashboard)
library(ggplot2)
library(plotly)
library(compmus)

library(cowplot)
library(patchwork)

Sys.setenv(SPOTIFY_CLIENT_ID="34c9d0d698064bf886a78b343db5445b")
Sys.setenv(SPOTIFY_CLIENT_SECRET="d0e93595f1ee411bb21938c5cb50d247")

access_token <- get_spotify_access_token()

pop <- get_playlist_audio_features("", "2AipD5zTjWWM13wk2hZmej")
phonk <- get_playlist_audio_features("", "37i9dQZF1DWWY64wDtewQt")
rap <- get_playlist_audio_features("", "37i9dQZF1DX76t638V6CA8")

awards <-
  bind_rows(
    pop |> mutate(category = "Pop"),
    phonk |> mutate(category = "Phonk"),
    rap |> mutate(category = "Rap")
    
  )
```




Introduction {.storyboard data-icon="ion-ios-home"}
=========================================

### Exploring the Unique Sounds: A Comparison of Three Distinct Music Genres
<font style="font-size: 30px">My Corpus choice</font>

My corpus consists of a collection of songs from various genres including mainly Pop, Rap, and Phonk (which is a sub genre of hip hop and
trap music). This corpus was chosen because I want to explore the differences and similarities among these various genres in terms of
their musical characteristics. Initially these three main genres within my corpus seem very different, so I thought it would be
interesting to see if there were any specific overlapping musical characteristics which makes them so compelling for my taste in music. Something interesting to keep in mind: I personally listen to these different genres under various circumstances. For example I tend to listen mostly to Pop when studying, while when working out I like to listen to Rap to give me that extra energy, and finally when gaming I tend to listen mostly to Phonk to focus in a game.

<font style="font-size: 30px">Comparisons</font>

The natural comparison points in my corpus are therefore the different genres: Pop, Rap, and Phonk. I expect to see differences in terms
of rhythm, melody, harmony, and lyrics. For example, I expected Pop music to have a more upbeat rhythm and catchy lyrics compared to Rap
music which might have a heavier beat and very different flowing lyrics. However, I also expected to see some similarities across genres,
for example Rap and Phonk often seem to have a similar type beat and rhythm: heavy and fast paced. It will be interesting to explore the
similarity and differences regarding the instruments and music keys/notes used in typical songs of each genre.

The tracks in my corpus are representative of the groups I want to compare, as I selected them based on their popularity and mainstream
recognition within each genre. However, it is possible that my corpus might not cover all sub-genres within each main genre, and therefore
may not represent the full scope of each genre.

Some examples of typical tracks in my corpus are "Invincible" by Pop Smoke, a popular Rap/Hiphop song, and "Counting Stars" by
OneRepublic, a popular Pop song with its iconic vocal harmonies and theatrical elements. Atypical tracks in my corpus include "Sweater
Weather" by the Neighbourhood, which is a mix between Indie rock and alternative rock, and "break from toronto" by PartyNextDoor, which
could be classified as R&B or Hiphop but also contains a form of rap. These atypical songs may cause there to be some outliers in the data.
These outliers may have to be filtered out first in order to be able to make the most fair comparison for each genre.

In conclusion, this portfolio will focus on exploring and analyzing the musical characteristics of Pop, Rap, and Phonk, to find out what
makes them appealing to listen to under various circumstances.

***

<iframe src="https://open.spotify.com/embed/playlist/2AipD5zTjWWM13wk2hZmej?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe src="https://open.spotify.com/embed/playlist/37i9dQZF1DWWY64wDtewQt?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe src="https://open.spotify.com/embed/playlist/37i9dQZF1DX76t638V6CA8?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


Visual analysis of my corpus {.storyboard data-icon="fa-signal"}
=========================================

### Keys & Genre
```{r}
p <- awards %>%
  ggplot(aes(x = key_name, fill = key_name)) +
  geom_bar() +
  facet_wrap(~category) +
  ggtitle("Distribution of keys per genre")

ggplotly(p)
```

***

This bar chart gives a good representation of the differences in key usage between the three genres. Something interesting to look at is the most used key for example, both Phonk and Rap make use of the C# key the most. However it seems that in Rap songs, this key is used almost twice as much as any other key on average, while in Phonk is seems to be relatively balanced with the other keys. It is also interesting to see that Phonk does not have a single instance of the D# key. Finally we can clearly see that out of the three genres, Pop seems to have the most balanced distribution of key usage since it uses all keys and does not really have one major outstanding key.


### Energy & Valence
```{r}
p <- awards |>                    # Start with awards.
  ggplot(                     # Set up the plot.
    aes(
      x = valence,
      y = energy,
      size = loudness,
      colour = energy
    )
  ) +
  geom_point() +              # Scatter plot.
  geom_rug(linewidth = 0.2) + # Add 'fringes' to show data distribution.

facet_wrap(~ category) +    # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_size_continuous(      # Fine-tune the sizes of each point.
    trans = "exp",            # Use an exp transformation to emphasise loud.
    guide = "none"            # Remove the legend for size.
  ) +
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Valence",
    y = "Energy",
  ) + ggtitle("Relationship between loudness, energy and valence in Phonk, Rap & Pop")

ggplotly(p)
```

***

For this second plot I decided to compare the valence and energy of The three genres: Pop, Rap and Phonk. Since I wanted to see if there was any correlation between the valence of and the energy of songs. I expected the Pop genre to have the highest average valence out of the three genres, since these songs are often linked to positive vibes. And Phonk to have the highest energy values on average, since this genre is associated with fast, loud and noisy music (as can be seen in the plot where bigger points indicate louder songs). Therefore it was no surprise to see that pop had indead a relatively high valence and Phonk had a relatively high energy. However I was surprised to find out that the Phonk genre actually also scored relatively high on average for valence too, since I assumed this fast and loud type of music would give off an angry vibe. This shows that there could be a correlation between the valence and energy of songs.


### Dancebility

```{r}
p <- awards %>%
  ggplot(aes(x = category, y = danceability, fill = category)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_brewer(palette="PuRd") +
  ggtitle("Dancebility: Phonk vs Pop vs Rap")

ggplotly(p)
```

***

This boxplot shows that interestingly enough, rap seems to have the highest danceability value on average according to spotify's measure. I would personally have guessed that Pop songs would have the highest danceability by a mile, so seeing Pop not scoring as high surprised me. I wonder what makes a song danceable by defenition of spotify, it would be interesting to see the exact features of this.



### Tempo & Genre

```{r}

fr <- pop |> slice(1:35) |>
  add_audio_analysis()

qu <- phonk |> slice(1:35) |>
  add_audio_analysis()

qu2 <- rap |> slice(1:35) |>
  add_audio_analysis()

comp <-
  fr |>
  mutate(category = "Pop") |>
  bind_rows(qu |> mutate(category = "Phonk"), qu2 |> mutate(category = "Rap"))
comp <- comp %>% rename(track = track.name)

Tempo <- comp |>
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections)

TempoAnimated <- Tempo %>%
  plot_ly(x = ~tempo, y = ~tempo_section_sd, color = ~tempo, size = ~loudness 
    ,hoverinfo = "text", text = paste("Track: ", Tempo$track, "<br>",
                          "Genre: ", Tempo$category, "<br>",
    #                       "Release date: ", Tempo$track.album.release_date, "<br>",
                          "Tempo: ", Tempo$tempo, "<br>",
                          "Loudness: ", Tempo$loudness, "<br>")
                          # "Time signature: ", Tempo$time_signature)
  ) %>%
  add_markers(frame = ~category) %>%
  layout(title = 'Tempo for Each Artist', xaxis = list(title = "Mean Tempo (bpm)"), yaxis = list(range = c(0,10), title = 'SD Tempo')) %>%
  animation_opts(frame = 2000, 
                 transition = 100,
                 easing = "linear") %>%
  animation_slider(currentvalue = list(prefix = NULL,
                                       font = list(color = "a9dea9", size = 20)))

TempoAnimated
```

***
**Explanation**  
Here you can see the mean tempo, the deviation of each song from that mean tempo, the loudness in size
and the bpm in the color of the point. The biggest deviation can be found in the Rap category as there are some outliers at the top of the plot. Pop seems to have the most stable bpm range: more than 50% lies within the 120-140 bpm range.


### Timbre & Genre
```{r}
viopl <- comp |>
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) |>
  select(category, timbre) |>
  compmus_gather_timbre() |>
  ggplot(aes(x = basis, y = value, fill = category)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Genre")

ggplotly(viopl)
```

***

**Explanation**
Timbre, the quality of sound that distinguishes different musical instruments, is a crucial aspect of music. The analysis of timbre is often performed using spectral content, which measures the relative strengths of various frequency components that make up the sound. In this study, the focus was on a specific timbre component, which was used to isolate and study the outliers in the corpus.

For every 12 levels of timbre used by Spotify API I have plotted the range for first 25 songs from each of the three genre-playlists: Pop, Phonk and Rap.



Chroma features {.storyboard data-icon="ion-music-note"}
=========================================

### Chromatograms

<!-- <font style="font-size: 30px">1. Counting Stars -- 2. Close Eyes -- 3. Invincible</font> -->

```{r pop-chroma}
ewf <-
  get_tidy_audio_analysis("2tpWsVSb9UEmDRxAl1zhX1") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
loveontop <-
  get_tidy_audio_analysis("3CLSHJv5aUROAN2vfOyCOh") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
allineed <-
  get_tidy_audio_analysis("792HwhrdO3ErRKL5yRe4Ge") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
```

```{r pop-chroma-plots}
ewf_plot <- ewf |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
    geom_vline(xintercept = 21.5, color = "red") +
    geom_vline(xintercept = 77.5, color = "red") +
    geom_vline(xintercept = 206.5, color = "red") +
  # labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Pop: 'Counting Stars' by 'OneRepublic'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  # theme_minimal() +
  scale_fill_viridis_c(option = "mako") 

loveontop_plot <- loveontop |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 70, color = "red") +
  # labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Phonk: 'Close Eyes' by 'DRVST'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme(legend.position="none") +
  scale_fill_viridis_c(option = "mako") 

allineed_plot <- allineed |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 23.5, color = "red") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "1.Pop: 'Counting Stars' by 'OneRepublic' --- 2. Phonk: 'Close Eyes' by 'DRVST' --- 3. Rap: 'Invincible' by 'Pop Smoke'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme(legend.position="none") +
  scale_fill_viridis_c(option = "mako") 

subplot(ewf_plot, loveontop_plot, allineed_plot, nrows = 3)
# plot_grid(ewf_plot, loveontop_plot, allineed_plot, ncol = 1)
```

***

A chromagram is a visual representation of the distribution of pitches in a musical recording. The first chromatogram shows that for the Pop song: "Counting Stars" by "OneRepublic", the C# key has the longest time use by far and it seems that the C key comes second. The rest of the keys do not seem to get much playtime in comparison.

The second chromatogram shows that for the Pop song: "Close Eyes" by "DRVST", the C key has the longest time use by far and it seems that the C#, F and E keys follow regarding playtime. The rest of the keys do not seem to get much playtime in comparison.

The third chromatogram shows that for the Pop song: "Invincible" by "Pop Smoke", the F# key has the longest time use by far, all the other keys do not even come close to having as much playtime.

Interestingly there seems to be some overlap between Pop and Phonk specific songs, whereby both genres seem to have the most playtime of the C and C# keys while the A#, A, D#, D keys all seem to have not much playtime. Furthermore it looks like the Rap song also makes the least use of the A# and D# keys, however these are the only comparable points since the F# key is being used the longest by far and the rest of keys being almost equally distributed.


### Cepstrograms


```{r timbre_pop}
# Cepstrograms
bzt <-
  get_tidy_audio_analysis("2tpWsVSb9UEmDRxAl1zhX1") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

cep_pop <- bzt |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +
  theme_classic()+
  ggtitle("1.Pop: 'Counting Stars' by 'OneRepublic' --- 2. Phonk: 'Close Eyes' by 'DRVST' --- 3. Rap: 'Invincible' by 'Pop Smoke'")
```


```{r timbre_phonk}
# Cepstrograms
bzt <-
  get_tidy_audio_analysis("3CLSHJv5aUROAN2vfOyCOh") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

cep_phonk <- bzt |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +
  theme_classic()
```


```{r timbre_rap}
# Cepstrograms
bzt <-
  get_tidy_audio_analysis("792HwhrdO3ErRKL5yRe4Ge") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

cep_rap <- bzt |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))

subplot(cep_pop, cep_phonk, cep_rap, nrows = 3)
```

***

The ceptrograms show the changes regarding timbre of typical songs from each genre: Pop, Phonk and Rap.
A ceptrogram is a visual representation of the distribution of timbral characteristics in a musical recording. Comparing the ceptrograms of three songs, “Counting Stars” from the Pop genre, “Close Eyes” from the Phonk genre and "Invincible"from the rap genre, reveals interesting differences in their timbral distribution. In “Counting Stars” there is a relavtively strong concentration of timbral characteristics around a certain range, indicating a pretty stable sonic texture. On the other hand, both “Invincible” and "Close Eyes" show more variation in timbral distribution, with a wider range of timbral characteristics. This suggests that these two have a more complex and varied sound texture. These differences in timbral distribution could contribute to the overall appeal of the songs and could be further explored in future analyses.


### Self-similarity matrices

```{r self sim, echo=FALSE}
norms_chroma <- list("euclidean", "manhattan", "chebyshev")
dists_chroma <- list("manhattan", "aitchison", "cosine", "angular")
summary_chroma <- list("mean", "aitchison_centre", "root_mean_square", "max", "acentre")

norms_timbre <- list("euclidean", "manhattan", "chebyshev")
dists_timbre <- list("euclidean", "cosine", "angular")
summary_timbre <- list("mean", "root_mean_square")

taste2 <-
  get_tidy_audio_analysis("2QjOHCTQ1Jl3zawyYOpxh6") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = summary_chroma[5], norm = norms_chroma[3]
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = summary_timbre[1]
      )
  )
```

```{r self sim matrices, echo=FALSE}
self_sim <- bind_rows(
  taste2 |> 
    compmus_self_similarity(pitches, dists_chroma[1]) |> 
    mutate(d = d / max(d), type = "Chroma"),
  taste2 |> 
    compmus_self_similarity(timbre, dists_timbre[2]) |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "mako", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")

self_sim_ggplotly <- ggplotly(self_sim, domain = list(x=c(0,1), y=c(0.6,1)))

parts <- data.frame(
  Chroma = c("0:00 - 0:23", "0:23 - 0:49", "0:49 - 1:12", "1:12 - 1:20", "1:20 - 2:28", ""), 
  Timbre = c("0:00 - 0:23", "0:23 - 1:12", "", "1:12 - 1:20", "1:20 - 1:24", "1:24 - 1:28"),
  Chroma2 = c("", "2:28 - 3:08", "3:08 - 4:11", "", "4:11 - 4:19", ""), 
  Timbre2 = c("1:28 - 2:28", "2:28 - 3:08", "3:08 - 3:35", "3:35 - 4:11", "4:11 - 4:19", ""))
```

```{r subplot, echo=FALSE}
parts_table <- plot_ly(
    type = 'table',
    domain = list(x=c(0,1), y=c(0,0.4)),
    header = list(values = c("Chroma", "Timbre", "Chroma", "Timbre")),
    cells = list(values = rbind(parts$Chroma, parts$Timbre, parts$Chroma2, parts$Timbre2), height=25)
  )
subplot(self_sim_ggplotly, parts_table, nrows=2)
```

***

A self-similarity matrix is a visual representation of the similarity between different sections of a musical recording. The two self-similarity matrices, each summarised at the bar level but with axes in seconds, illustrate pitch- and timbre-based self-similarity within the song sweater weather.






### Keygram of __XXXTENTACION : SAD!__

```{r keygram setup, echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}
#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)
major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r keychram corpus setup, echo=FALSE}
sad <-
  get_tidy_audio_analysis("5UVsbUV0Kh033cqsZ5sLQi") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r keygram, echo=FALSE}
sad1 <- sad |>
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
ggplotly(sad1)
```

***

This is a keygram from one of the last songs from _XXXTENTACION_: [SAD!](https://open.spotify.com/track/3ee8Jmje8o58CHK66QrVC2?si=450dd927a69547c9) brought out before he died the 18th of june 2018.



### temporal features

```{r}
tempf <- get_tidy_audio_analysis('5fEB6ZmVkg63GZg9qO86jh') |> 
    tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |> 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
    labs(x = 'Time (s)', y = 'Tempo (BPM)') +
    theme_classic()
ggplotly(tempf)
```

***

The Fourier-based tempogram at the left is an attempt to use Spotify's API to analyse the tempo of _PARTYNEXTDOOR_: [Break from Toronto](https://open.spotify.com/track/5fEB6ZmVkg63GZg9qO86jh?si=83b0b977075048e2) Overall, Spotify estimates that the tempo of this track is 117 BPM, looking in more detail at the tempogram, we can see that this makes sense.The bright yellow line does indeed lie around 117 BPM, however we can also see that there are some indicators that higher and lower tempos are also included in the beginning. These different sections of the piece lead to different strengths of so-called *tempo octaves*. This freer-form section of the piece can make it so tempo estimation is a little harder.



What did I find {.storyboard data-icon="fa-spotify"}
=========================================


### Conclusion


### Discussion

